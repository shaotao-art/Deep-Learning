{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## import module..."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn  # used to build network\n",
    "\n",
    "import torch.optim as optim # optimizer\n",
    "from torch.utils.data import DataLoader,Dataset # build dataset and dataloader\n",
    "\n",
    "import torchvision.transforms.functional as func\n",
    "import torchvision.transforms as transforms # transform to do data augmentation\n",
    "from PIL import Image # PIL to read image\n",
    "\n",
    "from tqdm import tqdm # progress bar\n",
    "import os # read img from file system\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## set random seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"./../src/img/unet.png\" width=720>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    down part of unet, follow the struct of: maxpool -> conv -> conv\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(DownBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            DoubleConv(in_channel, out_channel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return  self.block(x)\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    up part of unet, follow the struct of: conv -> conv -> up conv\n",
    "    mind that: the first conv get its input from both previous layer and down part's output\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, inter_channel, out_channel):\n",
    "        super(UpBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            DoubleConv(in_channel, inter_channel),\n",
    "            nn.ConvTranspose2d(inter_channel, out_channel, 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, down_part_data):\n",
    "        x = torch.cat((x, down_part_data), dim=1)\n",
    "        return self.block(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    model to do segmentation, out_dim represent the num class in segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_channel, out_dim):\n",
    "        super(UNet, self).__init__()\n",
    "        self.img_channel = img_channel\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "\n",
    "        self.init = DoubleConv(3, 64)\n",
    "        self.down1 = DownBlock(64, 128)\n",
    "        self.down2 = DownBlock(128,256)\n",
    "        self.down3 = DownBlock(256, 512)\n",
    "        self.down4 = DownBlock(512, 1024)\n",
    "\n",
    "        self.bottle_neck = nn.ConvTranspose2d(1024, 512, 2, 2)\n",
    "\n",
    "        self.up1 = UpBlock(1024, 512, 256)\n",
    "        self.up2 = UpBlock(512, 256, 128)\n",
    "        self.up3 = UpBlock(256, 128, 64)\n",
    "        self.up4 = DoubleConv(128, 64)\n",
    "\n",
    "        # final conv's is a conv1x1\n",
    "        self.final_conv = nn.Conv2d(64, out_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.init(x)\n",
    "        d2 = self.down1(d1)\n",
    "        d3 = self.down2(d2)\n",
    "        d4 = self.down3(d3)\n",
    "        d5 = self.down4(d4)\n",
    "        bottle = self.bottle_neck(d5)\n",
    "\n",
    "        u1 = self.up1(bottle, func.resize(d4, [56,56]))\n",
    "        u2 = self.up2(u1, func.resize(d3, [104, 104]))\n",
    "        u3 = self.up3(u2, func.resize(d2, [200, 200]))\n",
    "        u4_input = torch.cat((u3, func.resize(d1, [392, 392])), dim=1)\n",
    "        u4 = self.up4(u4_input)\n",
    "        return self.final_conv(u4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 56, 56]) torch.Size([8, 512, 56, 56])\n",
      "torch.Size([8, 256, 104, 104]) torch.Size([8, 256, 104, 104])\n",
      "torch.Size([8, 128, 200, 200]) torch.Size([8, 128, 200, 200])\n",
      "torch.Size([8, 2, 388, 388])\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    model = UNet(3, 2)\n",
    "    x = torch.randn(8, 3, 572, 572)\n",
    "    print(model(x).shape)\n",
    "test_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## define params"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "IMAGE_CHANNEL = 3\n",
    "OUT_DIM = 2\n",
    "NUM_EPOCH = 5\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = UNet(IMAGE_CHANNEL, OUT_DIM).to(DEVICE) # instance model\n",
    "# define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# define loss function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# define checkpoint for model saving\n",
    "model_checkpoint = {\n",
    "    'model':None,\n",
    "    'optimizer':None,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## prepare data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class CarDataset(Dataset):\n",
    "    def __init__(self, transform, data_path=\"./../dataset/segmentation/\"):\n",
    "        self.data_path = data_path\n",
    "        self.img_lst = os.listdir(self.data_path + \"train/\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_lst[idx]\n",
    "        x_path = self.data_path + \"train/\" + img_name\n",
    "        y_path = (self.data_path + \"train_masks/\" + img_name).replace(\".jpg\", \"_mask.gif\")\n",
    "        x = Image.open(x_path).convert(\"RGB\")\n",
    "        y = Image.open(y_path).convert(\"L\")\n",
    "        if self.transform:\n",
    "            x = self.transform[\"x_transform\"](x)\n",
    "            y = self.transform[\"y_transform\"](y)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_lst)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "\"x_transform\" : transforms.Compose([\n",
    "    transforms.Resize((572, 572)),\n",
    "    transforms.ToTensor()\n",
    "]),\n",
    "\"y_transform\" : transforms.Compose([\n",
    "    transforms.Resize((388, 388)),\n",
    "    transforms.ToTensor()\n",
    "])}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "train_dataset = CarDataset(data_transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCH):\n",
    "    loop = tqdm(train_dataloader, leave=True)\n",
    "    for batch_idx, (x, y) in enumerate(loop):\n",
    "        model.train()\n",
    "        ### define training loop here  ###\n",
    "        pred = model(x)\n",
    "        loss = criterion(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        ##################################\n",
    "        if batch_idx % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                # check acc\n",
    "                check_acc()\n",
    "\n",
    "    # saving models\n",
    "    model_checkpoint['model'] = model.state_dict()\n",
    "    model_checkpoint['optimizer'] = optimizer.state_dict()\n",
    "    save_checkpoint(model_checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat(\"./../dataset/human_pose/joints.mat\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "data=mat[\"joints\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  0.        , -26.10911452,   0.        ],\n       [ 26.03094352,  86.41022512,   1.        ],\n       [ 50.60790641,  75.70079791,   1.        ],\n       [ 71.72971103,  87.56715594,   1.        ],\n       [ 49.16955998,  89.00550237,   1.        ],\n       [ 23.13861646,  94.79015649,   1.        ],\n       [131.31164842,  13.80499887,   1.        ],\n       [111.64382442,  34.62975369,   1.        ],\n       [ 93.71139667,  49.09138898,   1.        ],\n       [102.38837784,  68.46216316,   1.        ],\n       [110.78394341,  88.42703696,   1.        ],\n       [139.12874857,  89.58396778,   1.        ],\n       [ 96.88513933,  56.61143933,   1.        ],\n       [116.56859752,  53.42206246,   1.        ]])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:,:,0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}