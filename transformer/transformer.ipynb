{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from einops import rearrange, repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![](https://mermaid.ink/img/pako:eNq1UjtPwzAQ_ivWTY2UVmLNwFQ2xNJuGEWmORqL2A7OGaja_nfOdltIeUzFQy6--x72J29h5RqECtZe9a1YzqUVvLTtA00kpCru70rRoS1Fo02t7YOEQkyn1zsJ1s5utUXlJ3mUIS5QIWEnXgL6DaukOlZhSJS5pNszRi_-_rvTq-oCskqqf7jFs7CeYEHyyg69G5D53K4pH3W2TPQDM-mM6DUdBR6NidaKCC1zU_20HvFy3D_w8jhTD2O-6MI9kVHvk-lVcULWb6jXLZ2M8nb4xfAr5czXpbjdedijmHKM33hxCCUY9Ebphp_oNnY4yhYNx1jxbxcdJUi7Z2DoG0V402hyHiryAUtQgdxiY1fHfcbMteLnbnJz_wHsB_sC)](https://mermaid.live/edit#pako:eNq1UjtPwzAQ_ivWTY2UVmLNwFQ2xNJuGEWmORqL2A7OGaja_nfOdltIeUzFQy6--x72J29h5RqECtZe9a1YzqUVvLTtA00kpCru70rRoS1Fo02t7YOEQkyn1zsJ1s5utUXlJ3mUIS5QIWEnXgL6DaukOlZhSJS5pNszRi_-_rvTq-oCskqqf7jFs7CeYEHyyg69G5D53K4pH3W2TPQDM-mM6DUdBR6NidaKCC1zU_20HvFy3D_w8jhTD2O-6MI9kVHvk-lVcULWb6jXLZ2M8nb4xfAr5czXpbjdedijmHKM33hxCCUY9Ebphp_oNnY4yhYNx1jxbxcdJUi7Z2DoG0V402hyHiryAUtQgdxiY1fHfcbMteLnbnJz_wHsB_sC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.to_q = nn.Linear(in_dim, out_dim)\n",
    "        self.to_k = nn.Linear(in_dim, out_dim)\n",
    "        self.to_v = nn.Linear(in_dim, out_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "        atten = torch.bmm(q, torch.permute(k, [0, 2, 1]))\n",
    "        atten = self.softmax(atten)\n",
    "        out = torch.bmm(atten, v)\n",
    "        return out\n",
    "\n",
    "def test_attention():\n",
    "    in_dim = 128\n",
    "    out_dim = 128\n",
    "    b_s = 8\n",
    "    sequence_len = 10\n",
    "    x = torch.randn(b_s, sequence_len, in_dim)\n",
    "    block = SelfAttention(in_dim, out_dim)\n",
    "    out = block(x)\n",
    "    make_dot(out)\n",
    "    assert block(x).shape == (b_s, sequence_len, out_dim)\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_head, in_dim):\n",
    "        super(MultiheadSelfAttention, self).__init__()\n",
    "        assert in_dim // num_head == in_dim / num_head\n",
    "        self.proj = nn.Linear(in_dim, in_dim)\n",
    "        self.num_head = num_head\n",
    "\n",
    "        inter_dim = in_dim // num_head\n",
    "        layers = [Attention(in_dim, inter_dim) for _ in range(num_head)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "        self.scale = inter_dim ** -0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = [self.layers[i](x) for i in range(self.num_head)]\n",
    "        outs = torch.cat(outs, dim=-1)\n",
    "        out = self.proj(outs)\n",
    "        return out\n",
    "\n",
    "def test_multiheadattention():\n",
    "    in_dim = 128\n",
    "    b_s = 8\n",
    "    sequence_len = 10\n",
    "    num_head = 8\n",
    "    x = torch.randn(b_s, sequence_len, in_dim)\n",
    "    block = MultiheadSelfAttention(num_head, in_dim)\n",
    "    assert block(x).shape == (b_s, sequence_len, in_dim)\n",
    "test_multiheadattention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        dim: input dim\n",
    "        num_head: #head in multihead attention\n",
    "        head_dim: dim of each head\n",
    "        dropout: default 0.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_head=8, head_dim=64, dropout=0.):\n",
    "        super(Attention, self).__init__()\n",
    "        self.inner_dim = num_head * head_dim\n",
    "        self.num_head = num_head\n",
    "        self.to_qkv = nn.Linear(dim, self.inner_dim * 3)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.proj = nn.Linear(self.inner_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            x: tensor (b_s, len_seq, dim)\n",
    "            \n",
    "        return:\n",
    "            out: tensor (b_s, len_seq, dim)\n",
    "        \"\"\"\n",
    "        # split into q k v\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        # reshape\n",
    "        q, k, v = map(lambda t:rearrange(t, \"b n (h d) ->  b h n d\", h=self.num_head), qkv)\n",
    "        # cal weights\n",
    "        weights = self.softmax(self.scale * torch.matmul(q, torch.transpose(k, -1, -2)))\n",
    "        weights = self.dropout(weights)\n",
    "        out = torch.matmul(weights, v)\n",
    "        # reshape\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "def test_attention():\n",
    "    in_dim = 128\n",
    "    b_s = 8\n",
    "    sequence_len = 10\n",
    "    x = torch.randn(b_s, sequence_len, in_dim)\n",
    "    block = Attention(in_dim)\n",
    "    assert block(x).shape == (b_s, sequence_len, in_dim)\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(Norm, self).__init__()\n",
    "        self.layer = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "def test_norm():\n",
    "    dim = 128\n",
    "    b_s = 8\n",
    "    sequence_len = 10\n",
    "    x = torch.randn(b_s, sequence_len, dim)\n",
    "    block = Norm(dim)\n",
    "    assert block(x).shape == (b_s, sequence_len, dim)\n",
    "test_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 65, 128])\n"
     ]
    }
   ],
   "source": [
    "class InputEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        img_h, img_w: h/w of input image\n",
    "        pathc_h, patch_w: h/w of patch\n",
    "        channel: channel of image\n",
    "        dim: dim of embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_h, img_w, patch_h, patch_w, channel, dim):\n",
    "        super(InputEmbed, self).__init__()\n",
    "        assert img_h // patch_h == img_h / patch_h\n",
    "        assert img_w // patch_w == img_w / patch_w\n",
    "        self.p_h = patch_h\n",
    "        self.p_w = patch_w\n",
    "        patch_dim = patch_h * patch_w * channel\n",
    "        self.patch_embed = nn.Linear(patch_dim, dim)\n",
    "\n",
    "        num_patch = (img_h // patch_h) * (img_w // patch_w)\n",
    "\n",
    "        # position embedding is just learnable params\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patch + 1, dim))\n",
    "\n",
    "        # class token for feature representation\n",
    "        self.cls_token =  nn.Parameter(torch.randn(1, 1, dim))\n",
    "\n",
    "\n",
    "    def forward(self, img):\n",
    "        \"\"\"\n",
    "        args:\n",
    "            img: tensor (b, c, h, w)\n",
    "        return:\n",
    "            out: tensor (b, num_patches + 1, dim)\n",
    "        \"\"\"\n",
    "\n",
    "        b_s = img.shape[0]\n",
    "        # split into patches\n",
    "        patches = rearrange(img, \"b c (p_h n_h) (p_w n_w) -> b (n_h n_w) (p_h p_w c) \", p_h=self.p_h, p_w = self.p_w)\n",
    "        patches = self.patch_embed(patches)\n",
    "\n",
    "        cls_token = repeat(self.cls_token, \"1 1 d -> b 1 d\", b=b_s)\n",
    "        features = torch.cat((patches, cls_token), dim=1)\n",
    "        pos_embed = repeat(self.pos_embed,\"1 n d -> b n d\", b=b_s)\n",
    "        features = features + pos_embed\n",
    "        return features\n",
    "\n",
    "\n",
    "def test_embed():\n",
    "    img_h, img_w = 64, 64\n",
    "    patch_h, patch_w = 8, 8\n",
    "    channel = 3\n",
    "    dim = 128\n",
    "    b_s = 8\n",
    "    num_patch = (img_h / patch_h) * (img_w / patch_w)\n",
    "    x = torch.randn(b_s, channel, img_h, img_w)\n",
    "    block = InputEmbed(img_h, img_w, patch_h, patch_w, channel, dim)\n",
    "    res = block(x)\n",
    "    assert res.shape == (b_s, num_patch + 1, dim)\n",
    "test_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, drop_out=0.):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_out),\n",
    "            nn.Linear(hidden_dim, in_dim),\n",
    "            nn.Dropout(drop_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "def test_MLP():\n",
    "    dim = 128\n",
    "    b_s = 8\n",
    "    sequence_len = 10\n",
    "    x = torch.randn(b_s, sequence_len, dim)\n",
    "    block = MLP(dim, 256)\n",
    "    assert block(x).shape == (b_s, sequence_len, dim)\n",
    "test_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_head, hidden_dim):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.norm1 = Norm(dim)\n",
    "        self.norm2 = Norm(dim)\n",
    "        self.fcs = MLP(dim, hidden_dim)\n",
    "        self.attention = MultiheadAttention(num_head, dim) \n",
    "\n",
    "    def forward(self, x):\n",
    "        resi = x\n",
    "        x = self.attention(self.norm1(x))\n",
    "        resi = x + resi\n",
    "        x = self.fcs(self.norm2(resi))\n",
    "        x = x + resi\n",
    "        return x\n",
    "        \n",
    "\n",
    "def test_MLP():\n",
    "    dim = 128\n",
    "    b_s = 8\n",
    "    sequence_len = 10\n",
    "    x = torch.randn(b_s, sequence_len, dim)\n",
    "    block = MLP(dim, 256)\n",
    "    assert block(x).shape == (b_s, sequence_len, dim)\n",
    "test_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(8, 3, 4)\n",
    "b = torch.randn(8, 4, 3)\n",
    "(a@b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "33a4a2141afdff6df85941b7817127327f4bc12be9b9d0589cd269c353a6e39b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
